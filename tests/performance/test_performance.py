import pytest\nimport time\nfrom typing import List\n\n\nclass TestPerformance:\n    \"\"\"Performance and load testing\"\"\"\n    \n    @pytest.mark.performance\n    def test_search_response_time(self, api_client):\n        \"\"\"Test that search responses are within acceptable time limits\"\"\"\n        search_queries = [\n            \"Dead and Company\",\n            \"Grateful Dead\",\n            \"Madison Square Garden\",\n            \"1995\"\n        ]\n        \n        response_times = []\n        \n        for query in search_queries:\n            start_time = time.time()\n            response = api_client.search_simple(query)\n            end_time = time.time()\n            \n            response_time = end_time - start_time\n            response_times.append(response_time)\n            \n            # Assert individual response time\n            assert response_time < 5.0, f\"Search for '{query}' took {response_time:.2f} seconds\"\n            assert \"albums\" in response\n        \n        # Assert average response time\n        avg_response_time = sum(response_times) / len(response_times)\n        assert avg_response_time < 3.0, f\"Average response time {avg_response_time:.2f} exceeds 3 seconds\"\n    \n    @pytest.mark.performance\n    def test_large_result_set_handling(self, api_client):\n        \"\"\"Test handling of queries that return large result sets\"\"\"\n        # Use broad search terms that might return many results\n        broad_queries = [\"the\", \"live\", \"2023\", \"rock\"]\n        \n        for query in broad_queries:\n            start_time = time.time()\n            response = api_client.search_simple(query)\n            end_time = time.time()\n            \n            response_time = end_time - start_time\n            \n            # Should still be fast even with large results\n            assert response_time < 10.0, f\"Large result query took {response_time:.2f} seconds\"\n            \n            # Verify result limits are enforced\n            for category in [\"albums\", \"artists\", \"tracks\", \"venues\"]:\n                if category in response:\n                    assert len(response[category]) <= 50\n    \n    @pytest.mark.performance \n    def test_sequential_requests_performance(self, api_client):\n        \"\"\"Test performance of sequential requests\"\"\"\n        queries = [\"test\"] * 10\n        \n        start_time = time.time()\n        \n        for i, query in enumerate(queries):\n            response = api_client.search_simple(f\"{query} {i}\")\n            assert \"albums\" in response\n        \n        end_time = time.time()\n        total_time = end_time - start_time\n        \n        # 10 requests should complete within reasonable time\n        assert total_time < 30.0, f\"10 sequential requests took {total_time:.2f} seconds\"\n        \n        avg_time_per_request = total_time / 10\n        assert avg_time_per_request < 3.0, f\"Average time per request: {avg_time_per_request:.2f} seconds\"\n    \n    @pytest.mark.performance\n    def test_concurrent_requests_performance(self, api_client):\n        \"\"\"Test handling of concurrent requests\"\"\"\n        import threading\n        \n        results = []\n        errors = []\n        response_times = []\n        \n        def make_request(query_id):\n            try:\n                start_time = time.time()\n                response = api_client.search_simple(f\"concurrent test {query_id}\")\n                end_time = time.time()\n                \n                response_times.append(end_time - start_time)\n                results.append(response)\n            except Exception as e:\n                errors.append(e)\n        \n        # Create multiple threads\n        threads = []\n        for i in range(5):\n            thread = threading.Thread(target=make_request, args=(i,))\n            threads.append(thread)\n            thread.start()\n        \n        # Wait for all threads to complete\n        for thread in threads:\n            thread.join()\n        \n        # Verify results\n        assert len(errors) == 0, f\"Concurrent requests failed: {errors}\"\n        assert len(results) == 5\n        \n        for result in results:\n            assert \"albums\" in result\n        \n        # Check that concurrent requests don't slow down significantly\n        avg_concurrent_time = sum(response_times) / len(response_times)\n        assert avg_concurrent_time < 10.0, f\"Average concurrent response time: {avg_concurrent_time:.2f}s\"\n    \n    @pytest.mark.performance\n    def test_memory_usage_stability(self, api_client):\n        \"\"\"Test that repeated requests don't cause memory leaks\"\"\"\n        import gc\n        import sys\n        \n        # Get initial memory usage (rough estimate)\n        gc.collect()\n        initial_objects = len(gc.get_objects())\n        \n        # Make many requests\n        for i in range(50):\n            response = api_client.search_simple(f\"memory test {i}\")\n            assert \"albums\" in response\n            \n            # Force garbage collection every 10 requests\n            if i % 10 == 0:\n                gc.collect()\n        \n        # Check final memory usage\n        gc.collect()\n        final_objects = len(gc.get_objects())\n        \n        # Memory usage shouldn't grow excessively (allow 20% increase)\n        memory_growth_ratio = final_objects / initial_objects\n        assert memory_growth_ratio < 1.2, f\"Memory usage grew by {memory_growth_ratio:.2f}x\""